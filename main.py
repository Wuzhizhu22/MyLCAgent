import yaml
import argparse
from dataclasses import dataclass
from pydantic import BaseModel, Field, field_validator

from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langchain.tools import tool, ToolRuntime
from langchain_core.callbacks import UsageMetadataCallbackHandler
from langchain.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.config import get_stream_writer

# --- é…ç½®ä¸å¸¸é‡ ---
SYSTEM_MESSAGE = SystemMessage(
    content="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¤©æ°”é¢„æŠ¥å‘˜ï¼Œè¯´è¯æ—¶å–œæ¬¢ç”¨åŒå…³è¯­ã€‚
    å¦‚æœç”¨æˆ·è¯¢é—®å¤©æ°”ï¼Œè¯·ç¡®ä¿çŸ¥é“ä½ç½®ã€‚è‹¥æŒ‡ä»£å½“å‰ä½ç½®ï¼Œè¯·å…ˆè·å–ä½ç½®å†æŸ¥è¯¢ã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ ReAct æ ¼å¼è¾“å‡ºï¼š<question>, <thought>, <action>, <observation>, <final_answer>ã€‚""",
    id="system_001",
)


@dataclass
class Context:
    user_id: str


class WeatherQuery(BaseModel):
    city: str = Field(description="åŸå¸‚åç§°")

    @field_validator("city")
    def city_must_not_be_empty(cls, v):
        if not v or v.strip() == "":
            raise ValueError("åŸå¸‚åç§°ä¸èƒ½ä¸ºç©º")
        return v.strip()


@dataclass
class ResponseFormat:
    question: str
    thought: str
    action: str
    observation: str
    final_answer: str


# --- å·¥å…·å®šä¹‰ ---
@tool(
    "get_weather_for_location",
    description="è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯ã€‚å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æˆ–éœ€è¦å¤©æ°”æ•°æ®æ—¶ä½¿ç”¨æ­¤å·¥å…·ï¼Œè¿”å›æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”çŠ¶å†µã€‚",
)
def get_weather_for_location(query: WeatherQuery) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
    writer = get_stream_writer()
    city = query.city.strip()
    writer(f"ğŸ” æ­£åœ¨æŸ¥è¯¢: {city}")
    return f"{city}æ€»æ˜¯é˜³å…‰æ˜åªšï¼ŒçœŸæ˜¯â€˜æ™´â€™æ·±ä¼¼æµ·ï¼"


@tool(
    "get_user_location",
    description="æ ¹æ®ç”¨æˆ·IDè·å–ç”¨æˆ·ä½ç½®ä¿¡æ¯ã€‚å½“éœ€è¦çŸ¥é“ç”¨æˆ·ä½ç½®æ—¶ä½¿ç”¨æ­¤å·¥å…·ï¼Œæ ¹æ®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ä¸­çš„ç”¨æˆ·IDè¿”å›å¯¹åº”çš„ä½ç½®ä¿¡æ¯ã€‚",
)
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """è·å–ç”¨æˆ·ä½ç½®"""
    writer = get_stream_writer()
    location = "ä½›ç½—é‡Œè¾¾" if runtime.context.user_id == "1" else "æ—§é‡‘å±±"
    writer(f"ğŸ“ æ‰¾åˆ°ä½ç½®: {location}")
    return location


# --- æ‰“å°è¾…åŠ©å‡½æ•° (åˆå¹¶ç®€åŒ–) ---
def print_react_step(step_type: str, content: str, tool_args: dict = None) -> None:
    """ç»Ÿä¸€çš„ ReAct æ­¥éª¤æ‰“å°å‡½æ•°"""
    styles = {
        "question": ("â“", "Question"),
        "thought": ("ğŸ’­", "Thought"),
        "action": ("ğŸ”§", "Action"),
        "observation": ("ğŸ”", "Observation"),
        "final_answer": ("âœ…", "Final Answer"),
    }
    icon, label = styles.get(step_type.lower(), ("ğŸ“„", step_type.title()))

    # è§£ç  Unicode è½¬ä¹‰åºåˆ—
    if content and "\\u" in content:
        try:
            content = content.encode("utf-8").decode("unicode_escape")
        except:
            pass  # å¦‚æœè§£ç å¤±è´¥ï¼Œä¿æŒåŸæ ·

    if step_type == "action" and tool_args:
        args_str = ", ".join(f"{k}={v}" for k, v in tool_args.items())
        print(f"\n{icon} {label}: {content}({args_str})")
    else:
        print(f"\n{icon} {label}: {content}")


def print_token_usage(callback: UsageMetadataCallbackHandler) -> None:
    """ç»Ÿè®¡ Token ä½¿ç”¨"""
    print(f"\nğŸ“Š Token ç»Ÿè®¡:")
    for model, meta in (callback.usage_metadata or {}).items():
        print(
            f"   {model} -> In: {meta.get('input_tokens')} | Out: {meta.get('output_tokens')} | Total: {meta.get('total_tokens')}"
        )


# --- æ ¸å¿ƒé€»è¾‘è§£æå™¨ ---
def dispatch_react_elements(message):
    """è§£ææ¶ˆæ¯å†…å®¹å¹¶åˆ†å‘ç»™æ‰“å°å‡½æ•°"""
    if isinstance(message, ToolMessage):
        if not message.content.startswith("Returning structured response"):
            print_react_step("observation", message.content)
        return

    if not isinstance(message, AIMessage):
        return

    # å¤„ç†æ€ç»´é“¾æˆ–ç»“æ„åŒ–å·¥å…·è°ƒç”¨
    if hasattr(message, "content_blocks") and message.content_blocks:
        for block in message.content_blocks:
            b_type = block.get("type")
            if b_type == "reasoning":
                summary = " ".join([item.get("text", "") for item in block.get("summary", [])])
                print_react_step("thought", summary)
            elif b_type == "tool_call":
                if block.get("name") == "ResponseFormat":
                    args = block.get("args", {})
                    for field in ["question", "thought", "final_answer"]:
                        if args.get(field):
                            print_react_step(field, args[field])
                else:
                    print_react_step("action", block.get("name"), block.get("args"))
            elif b_type == "text" and block.get("text"):
                print_react_step("info", block["text"])


# --- æ‰§è¡Œæ¨¡å¼ ---
def run_agent(agent, user_input, config, context, mode="invoke"):
    print(f"\nğŸ¤” æ™ºèƒ½ä½“æ­£åœ¨æ€è€ƒ ({mode})...\n")
    user_msg = HumanMessage(content=user_input, name=f"user_{context.user_id}")

    if mode == "stream":
        for stream_mode, chunk in agent.stream(
            {"messages": [user_msg]}, stream_mode=["updates", "custom"], config=config, context=context
        ):
            if stream_mode == "custom":
                print(f"ğŸ¯ {chunk}")
            elif stream_mode == "updates":
                for data in chunk.values():
                    if not data:
                        continue
                    if "structured_response" in data:
                        sr = data["structured_response"]
                        for f in ["question", "thought", "final_answer"]:
                            if hasattr(sr, f):
                                print_react_step(f, getattr(sr, f))
                    else:
                        for m in data.get("messages", []):
                            dispatch_react_elements(m)
    else:
        result = agent.invoke({"messages": [user_msg]}, config=config, context=context)
        for m in result.get("messages", []):
            dispatch_react_elements(m)


# --- ä¸»ç¨‹åº ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--conversation", action="store_true")
    parser.add_argument("--output-mode", choices=["stream", "invoke"], default="invoke")
    parser.add_argument("--show-tokens", action="store_true")
    args = parser.parse_args()

    with open("./llm.yaml", "r") as f:
        llm_config = yaml.safe_load(f)

    model = ChatOpenAI(**llm_config["llm"])
    agent = create_agent(
        model=model,
        system_prompt=SYSTEM_MESSAGE.content,
        tools=[get_user_location, get_weather_for_location],
        context_schema=Context,
        response_format=ResponseFormat,
        checkpointer=InMemorySaver(),
        middleware=[SummarizationMiddleware(model=model, trigger=("tokens", 40000), keep=("messages", 20))],
    )

    token_cb = UsageMetadataCallbackHandler()
    config = {"configurable": {"thread_id": "1"}, "callbacks": [token_cb]}
    ctx = Context(user_id="1")

    if args.conversation:
        print("ğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºå¯¹è¯")
        while True:
            inp = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
            if inp.lower() in ["exit", "quit", "é€€å‡º"]:
                break
            if not inp:
                continue
            run_agent(agent, inp, config, ctx, args.output_mode)
            if args.show_tokens:
                print_token_usage(token_cb)
    else:
        inp = input("ğŸ‘¤ è¯·è¾“å…¥é—®é¢˜: ").strip() or "å¤©æ°”å¦‚ä½•å‘¢?"
        run_agent(agent, inp, config, ctx, args.output_mode)
        if args.show_tokens:
            print_token_usage(token_cb)


if __name__ == "__main__":
    main()
