import yaml
import argparse

from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.config import get_stream_writer
from langchain_core.callbacks import UsageMetadataCallbackHandler
from langchain.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage

from dataclasses import dataclass
from pydantic import BaseModel, Field, field_validator

SYSTEM_MESSAGE = SystemMessage(
    content="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¤©æ°”é¢„æŠ¥å‘˜ï¼Œè¯´è¯æ—¶å–œæ¬¢ç”¨åŒå…³è¯­ã€‚

å¦‚æœç”¨æˆ·å‘ä½ è¯¢é—®å¤©æ°”ï¼Œç¡®ä¿ä½ çŸ¥é“ä½ç½®ã€‚å¦‚æœä»é—®é¢˜ä¸­å¯ä»¥çœ‹å‡ºä»–ä»¬æŒ‡çš„æ˜¯ä»–ä»¬æ‰€åœ¨çš„ä½ç½®ï¼Œè¯·å…ˆè·å–ç”¨æˆ·ä½ç½®ï¼Œç„¶åæŸ¥è¯¢å¤©æ°”ã€‚""",
    id="system_001",
)


@dataclass
class Context:
    """è‡ªå®šä¹‰è¿è¡Œæ—¶ä¸Šä¸‹æ–‡æ¨¡å¼ã€‚"""

    user_id: str


class WeatherQuery(BaseModel):
    """å¤©æ°”æŸ¥è¯¢å‚æ•°ï¼ˆä½¿ç”¨Pydanticï¼‰"""

    city: str = Field(description="åŸå¸‚åç§°æˆ–åœ°åŒºï¼Œä¾‹å¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€çº½çº¦ã€ä½›ç½—é‡Œè¾¾")

    @field_validator("city")
    def city_must_not_be_empty(cls, v):
        if not v or v.strip() == "":
            raise ValueError("åŸå¸‚åç§°ä¸èƒ½ä¸ºç©º")
        return v.strip()


@tool(
    "get_weather_for_location",
    description="è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯ã€‚å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æˆ–éœ€è¦å¤©æ°”æ•°æ®æ—¶ä½¿ç”¨æ­¤å·¥å…·ï¼Œè¿”å›æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”çŠ¶å†µã€‚",
)
def get_weather_for_location(query: WeatherQuery) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
    writer = get_stream_writer()
    writer(f"ğŸ” æ­£åœ¨æŸ¥è¯¢åŸå¸‚: {query.city}")
    writer(f"ğŸ“Š è·å–åˆ°åŸå¸‚æ•°æ®: {query.city}")
    return f"{query.city}æ€»æ˜¯é˜³å…‰æ˜åªšï¼"


@tool(
    "get_user_location",
    description="æ ¹æ®ç”¨æˆ·IDè·å–ç”¨æˆ·ä½ç½®ä¿¡æ¯ã€‚å½“éœ€è¦çŸ¥é“ç”¨æˆ·ä½ç½®æ—¶ä½¿ç”¨æ­¤å·¥å…·ï¼Œæ ¹æ®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ä¸­çš„ç”¨æˆ·IDè¿”å›å¯¹åº”çš„ä½ç½®ä¿¡æ¯ã€‚",
)
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """æ ¹æ®ç”¨æˆ·IDè·å–ç”¨æˆ·ä½ç½®ä¿¡æ¯"""
    writer = get_stream_writer()
    user_id = runtime.context.user_id
    writer(f"ğŸ‘¤ æ­£åœ¨æŸ¥æ‰¾ç”¨æˆ·ID: {user_id}")
    location = "ä½›ç½—é‡Œè¾¾" if user_id == "1" else "æ—§é‡‘å±±"
    writer(f"ğŸ“ æ‰¾åˆ°ç”¨æˆ·ä½ç½®: {location}")
    return location


@dataclass
class ResponseFormat:
    """å“åº”æ ¼å¼ï¼šåŒ…å«åŒå…³è¯­å›ç­”å’Œå¯é€‰çš„å¤©æ°”çŠ¶å†µ"""

    punny_response: str
    weather_conditions: str | None = None

    def __str__(self):
        result = f"å›ç­”ï¼š{self.punny_response}"
        if self.weather_conditions:
            result += f"\nå¤©æ°”çŠ¶å†µï¼š{self.weather_conditions}"
        return result


def load_config(config_path):
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def create_human_message(content: str, user_id: str = None) -> HumanMessage:
    """åˆ›å»ºå¸¦å…ƒæ•°æ®çš„äººç±»æ¶ˆæ¯"""
    return HumanMessage(content=content, name=f"user_{user_id}" if user_id else "user")


def process_message_blocks(message: AIMessage) -> dict:
    """å¤„ç† AIMessage çš„ content_blocksï¼Œæå–ä¸åŒç±»å‹çš„å†…å®¹"""
    result = {"tool_calls": [], "text_content": [], "reasoning": None}

    content_blocks = message.content_blocks

    if content_blocks:
        for block in content_blocks:
            block_type = block.get("type")

            if block_type == "tool_call":
                result["tool_calls"].append(
                    {"name": block.get("name"), "args": block.get("args"), "id": block.get("id")}
                )
            elif block_type == "text":
                result["text_content"].append(block.get("text"))
            elif block_type == "reasoning":
                result["reasoning"] = block.get("summary", [])

    return result


tool_config = load_config("./llm.yaml")
model = ChatOpenAI(**tool_config["llm"])


def main():
    parser = argparse.ArgumentParser(description="å¤©æ°”æŸ¥è¯¢æ™ºèƒ½ä½“")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["stream", "invoke"],
        default="invoke",
        help="è¿è¡Œæ¨¡å¼ï¼šstreamï¼ˆæµå¼è¾“å‡ºï¼‰æˆ– invokeï¼ˆä¸€æ¬¡æ€§è¾“å‡ºï¼‰",
    )
    args = parser.parse_args()

    checkpointer = InMemorySaver()
    agent = create_agent(
        model=model,
        system_prompt=SYSTEM_MESSAGE.content,
        tools=[get_user_location, get_weather_for_location],
        context_schema=Context,
        response_format=ResponseFormat,
        checkpointer=checkpointer,
    )

    callback = UsageMetadataCallbackHandler()
    config_with_callback = {"configurable": {"thread_id": "1"}, "callbacks": [callback]}

    user_message = create_human_message("å¤©æ°”å¦‚ä½•å‘¢?", user_id="1")

    if args.mode == "stream":
        print("=== å¼€å§‹æµå¼è¾“å‡º ===\n")

        for stream_mode, chunk in agent.stream(
            {"messages": [user_message]},
            stream_mode=["updates", "custom"],
            config=config_with_callback,
            context=Context(user_id="1"),
        ):
            print(f"ğŸ“¡ æµæ¨¡å¼: {stream_mode}")

            if stream_mode == "custom":
                print(f"  ğŸ¯ {chunk}")
            elif stream_mode == "updates":
                for step, data in chunk.items():
                    print(f"ğŸ“ æ­¥éª¤: {step}")

                    messages = data.get("messages", [])
                    if messages:
                        last_message = messages[-1]

                        if isinstance(last_message, AIMessage):
                            blocks = process_message_blocks(last_message)

                            for tool_call in blocks["tool_calls"]:
                                print(f"  ğŸ› ï¸  è°ƒç”¨å·¥å…·: {tool_call['name']}")
                                print(f"  ğŸ“ å‚æ•°: {tool_call['args']}")

                            for text in blocks["text_content"]:
                                print(f"  ğŸ’¬ å†…å®¹: {text}")

                            if blocks["reasoning"]:
                                print(f"  ğŸ§  æ¨ç†è¿‡ç¨‹:")
                                for summary in blocks["reasoning"]:
                                    print(f"    - {summary.get('text', '')}")
                        else:
                            print(f"  ğŸ“„ æ¶ˆæ¯ç±»å‹: {type(last_message).__name__}")
                            print(f"  ğŸ“„ æ¶ˆæ¯å†…å®¹: {last_message.content}")

                    print()

        print("\n=== æµå¼è¾“å‡ºå®Œæˆ ===")
    else:
        print("=== ä½¿ç”¨ invoke æ¨¡å¼ ===\n")

        result = agent.invoke(
            {"messages": [user_message]},
            config=config_with_callback,
            context=Context(user_id="1"),
        )

        print(result["structured_response"])
        print("\n=== invoke å®Œæˆ ===")

    print("\nğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡:")
    if callback.usage_metadata:
        for model_name, metadata in callback.usage_metadata.items():
            print(f"  æ¨¡å‹: {model_name}")
            print(f"    è¾“å…¥ Tokens: {metadata.get('input_tokens', 0)}")
            print(f"    è¾“å‡º Tokens: {metadata.get('output_tokens', 0)}")
            print(f"    æ€»è®¡ Tokens: {metadata.get('total_tokens', 0)}")
    else:
        print("  æœªè·å–åˆ° token ä½¿ç”¨ç»Ÿè®¡")
        print(f"  å®Œæ•´ metadata: {callback.usage_metadata}")


if __name__ == "__main__":
    main()
